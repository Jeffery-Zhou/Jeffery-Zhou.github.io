
<!DOCTYPE HTML>
<html lang="en" xmlns="http://www.w3.org/1999/html"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Peng Zhou </title>

  <meta name="author" content="Peng Zhou">
  <meta name="descripction" content="Peng Zhou is a post-doctoral researcher at the university of hong kong.">
  <meta name="keywords" content="Peng Zhou, post-doc, robotics, HKU">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="imgs/hku-icon-small.jpg">

  <script src="function.js"></script>
</head>

<body data-gr-c-s-loaded="true">
<table width="840" border="0" align="center" cellspacing="0" cellpadding="20"><tbody><tr><td>

  <!-- title -->

  <p align="center">
    <pageheading>Peng Zhou 
      <!-- <a href="./index_CN.html" target="_blank">‰∏≠Êñá‰∏ªÈ°µ</a>
    </pageheading><br> -->
  </p>

  <!-- avatar and bio -->

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr>
    <td width="30%" valign="center" style="text-align: center;">
      <img src="imgs/me3.jpg" width="80%" style="border-radius:20px">
      <p align="center">
        | <a href="https://scholar.google.com/citations?user=y8CcXFoAAAAJ">Google Scholar</a> | <a href="https://github.com/Jeffery-Zhou">Github</a> |
<!--        <br> | <a href="https://www.linkedin.com/in/tete-xiao-ba2103120/"> Linkedin</a> | <a href="pdfs/resume.pdf">CV</a> | <a href="https://twitter.com/tetexiao_ai">Twitter</a> |-->
      </p>
    </td>

    <td width="70%" valign="center" align="justify">
      <p>
        Currently, I am an assistant professor with the School of Advanced Engineering of The Great Bay University, 
        and the Principal Investigator of the <a href="https://email-lab.github.io/">Embodied MAnipulation InteLligence ÔºàEMAILÔºâ </a> Robotics Lab.  My research interests lie in the fields of <i>robotics</i>, <i>machine learning</i> and <i>computer vision</i>, with a focus on <strong>deformable object manipulation</strong>, <strong>robot perception and learning</strong> and <strong>task and motion planning</strong>.

<!--         I will be joining <a href="https://www.gbu.edu.cn/?lang=en" target="_blank">Great Bay University</a> as an Assistant Professor in the Spring of 2025. -->
      </p>

      <!-- <p>
        My research interests lie in the fields of <i>robotics</i>, <i>machine learning</i> and <i>computer vision</i>, with a focus on <strong>deformable object manipulation</strong>, <strong>robot perception and learning</strong> and <strong>task and motion planning</strong>.

       <u>My ultimate research goal üåà</u> is to <u> develop robots ü§ñ with human-like cognition üß† and manipulation ü¶æ abilities.</u>
      </p> -->


      <p>
        Before that, I worked at the <a href='https://www.romi-lab.org/'> Robotic and Machine Intelligence (ROMI) Lab</a> and received my <b>Ph.D. degree in Robotics</b> from The Hong Kong Polytechnic University, under the supervision of <a href='https://www.polyu.edu.hk/me/david/'> Dr. David Navarro-Alarcon</a>.
        I also worked as a Postdoctoral Research Fellow at the University of Hong Kong (HKU) advised by <a href='https://www.cs.hku.hk/people/academic-staff/jpan'> Dr. Pan Jia</a>.
        In 2021, I visited the <a href='https://www.kth.se/is/rpl/about-rpl'>  Robotics, Perception and Learning (RPL) Lab </a> at KTH as an exchange Ph.D. student under the supervision of <a href='https://www.csc.kth.se/~danik/'> Prof. Danica Kragic </a>. Furthermore, during my Ph.D. study and subsequent research, I had the opportunity to collaborate with
        <a href='https://jihong-zhu.github.io/'> Dr. Jihong Zhu </a>,
        <a href='https://irmv.sjtu.edu.cn/wanghesheng'> Prof. Hesheng Wang </a>,
        <a href='https://www.polyu.edu.hk/ise/people/academic-staff/pai-zheng/'> Dr. Pai Zheng </a>
        and
        <a href='https://people.uwe.ac.uk/Person/CharlieYang'> Prof. Charlie Yang </a>.
      </p>
      
<!--      <p>-->
<!--        Prior to PolyU, I had two years with Tencent and received a M.S. degree in Software Engineering from Tongji University in 2017 with the Outstanding Graduate honor.-->
<!--      </p>-->
    </td>
  </tr>
</tbody>
</table>



 <!-- CfP -->

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody>
    <tr>
      <td>
        <sectionheading>Call for Papers</sectionheading>
      </td>
    </tr>
    <tr>
      <td>
        <div style="background: #e8f4ff; border: 2px solid #2691d9; border-radius: 8px; padding: 18px 20px; margin: 12px 0;">
          <ul style="margin: 0; padding-left: 20px; list-style: none;">
            <li style="margin-bottom: 12px; white-space: nowrap; overflow-x: auto;">
              <span style="font-size: 1.2em; color: #2691d9;">üì¢</span>
              <b>[Workshop]</b>
              <a href="https://sites.google.com/view/iros-2025-workshop-cim" style="color: #1565c0;">
                Workshop on Contact and Impact-aware Manipulation</a> @ 
              <a href="https://www.iros25.org/">
                <img src="https://www.iros25.org/templates/iros2025/static/images/logo.png" alt="IROS 2025 Logo" style="height:38px;vertical-align:middle;margin-right:3px;">
              </a> 
            </li>
            <li style="margin-bottom: 12px; white-space: nowrap; overflow-x: auto;">
              <span style="font-size: 1.2em; color: #2691d9;">üì¢</span>
              <b>[Special Issue]</b>
              <a href="https://www.frontiersin.org/research-topics/70782/enhancing-robotic-dexterity-through-advanced-learning-and-multimodal-perception" target="_blank" style="color: #1565c0;">
                Enhancing robotic dexterity through advanced learning and multimodal perception
              </a>
              @ 
              <img src="https://www.european-mrs.com/sites/default/files/field/image/logo_frontiers_grey.png" alt="Frontiers Logo" style="height:28px;vertical-align:middle;margin-right:3px;">
            </li>

            <!-- <li style="margin-bottom: 12px; white-space: nowrap; overflow-x: auto;">
              <span style="font-size: 1.2em; color: #2691d9;">üì¢</span>
              <b>[Special Issue]</b>
              <a href="https://www.mdpi.com/journal/electronics/special_issues/45780SII8Z" target="_blank" style="color: #1565c0;">
                Advancements in Robotics: Perception, Manipulation, and Interaction
              </a>
              @ 
              <img src="https://www.mdpi.com/img/journals/electronics-logo.png" alt="Electronics Logo" style="height:28px;vertical-align:middle;margin-right:3px;">
            </li> -->

            <!-- <li style="white-space: nowrap; overflow-x: auto;">
              <span style="font-size: 1.2em; color: #2691d9;">üì¢</span>
              <b>[Special Issue]</b>
              <a href="https://www.mdpi.com/journal/electronics/special_issues/P08ZQW9WAG" target="_blank" style="color: #1565c0;">
                Research on Deep Learning and Human-Robot Collaboration
              </a>
              @ 
              <img src="https://www.mdpi.com/img/journals/electronics-logo.png" alt="Electronics Logo" style="height:28px;vertical-align:middle;margin-right:3px;">
            </li> -->

          </ul>
        </div>
      </td>
    </tr>
  </tbody>
</table>

  <!-- news -->

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tbody>
    <tr>
      <sectionheading>&nbsp;&nbsp;News</sectionheading>

      <td>
      <ul>

        <!-- <li> <b>[Aug. 2025] </b> Assigned as a <strong> Program Committee</strong> for International Symposium on Embodied intelligence and Humanoid Robots (ROBOI 2025) </li> -->

        
        <!-- <li> <b>[Aug. 2025] </b> Assigned as an <strong> Organizing Committee Chair </strong> for 2025 6th International Conference on Artificial Intelligence Applications and Technologies (AIAAT 2025) </li> -->

        <!-- <li> <b>[Aug. 2025] </b> Assigned as a <strong> Program Committee</strong> for IThe International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR 2025) </li>   -->

        <li>
          <b>[July 2025]</b> Dr. Zhou is serving as an <b>Associate Editor</b> for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7083369">IEEE Robotics and Automation Letters</a>.
        </li>

        <li>
          <b>[July 2025]</b> <a href="https://ieeexplore.ieee.org/abstract/document/11016126"> Explicit-Implicit Subgoal Planning for Long-Horizon Tasks with Sparse Rewards</a>, IEEE Transactions on Automation Science and Engineering.
        </li>

        <li>
          <b>[July 2025]</b> <a href="https://ieeexplore.ieee.org/abstract/document/11018378">A Joint Learning of Force Feedback of Robotic Manipulation and Textual Cues for Granular Materials Classification</a>, IEEE Robotics and Automation Letters.
        </li>

        <li>
          <b>[Jun. 2025]</b> <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=11009179">Iterative Shaping of Multi-Particle Aggregates based on Action Trees and VLM</a>, IEEE Robotics and Automation Letters.
        </li>

        <li>
          <b>[Apr. 2025]</b> <a href="https://sites.google.com/view/iros-2025-workshop-cim">Workshop on Contact and Impact-aware Manipulation (IROS 2025)</a> is accepted by IROS 2025.
        </li>



        <!-- hidden News -->
        <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
        <div id="old_news" style="display: none;">

        <li>
          <b>[Nov. 2024] </b> Our Omni-mani team won the <strong> champion üèÜ in Track 3:
          <a href=" https://www.bilibili.com/video/BV1aSDGYREKg/?vd_source=e3b2332677ebeb75a29ff75f04959d2f">
            Robot Task for Organizing and Plugging/Unplugging Messy Cables
          </a>
        </strong> at the <i>Zhuhai International Dexterous Manipulation Challenge</i>.
        </li>

        <li> <b>[Oct. 2024] </b> Our new paper on
          <a href="https://arxiv.org/abs/2401.11432">Deformable Bag Manipulation using Neural particle-based dynamics model</a>
          accepted to <i>IEEE/ASME Transactions on Mechatronics</i>. </li>  

        <li> <b>[Oct. 2024] </b> Our new paper on
          <a href="https://ieeexplore.ieee.org/document/10605907">Interactive Perception for Deformable Bag Manipulation </a>
          accepted to <i>IEEE Robotics and Automation Letters</i>. </li>

        <li> <b>[Jul. 2024] </b> Served as a <strong> Guest Editor</strong> for Special Issue
          <a href="https://www.mdpi.com/journal/electronics/special_issues/45780SII8Z">Advancements in Robotics: Perception, Manipulation, and Interaction </a>
          with <i>Electronics</i>.</li>

        <li> <b>[Dec. 2023]</b> Our new paper on
          <a href="https://ieeexplore.ieee.org/abstract/document/10378708">Imitating Learning for Tool-based Garment Folding </a>
          accepted to <i>IEEE Transactions on Industrial Informatics</i>.</li>  

        <li> <b>[Jun. 2023] </b> Assigned as an <strong> Associate Editor</strong> for IEEE International Conference on Robotics and Biomimetics (ROBIO 2023) </li>  

        <li> <b>[Nov. 2022] </b> Our team Won <strong> Best AI Implementation Award </strong> in <i>AI x HK Opencup 2022. </i> </li>

        <li> <b>[Oct. 2022] </b> Won <strong> Outstanding Young Research Award  </strong>, <i>CNERC Annual Technical Symposium 2022.</i> </li>
<!--        <li> <b> </b> </li>-->

        </div>

      </ul>

    </td>

<!--      <td width="22%" valign="center">-->
<!--        <img src="imgs/zhuhai.jpg" width="100%" style="border-radius:20px">-->
<!--      </td>-->

    </tr>
    </tbody>
  </table>

  <!-- publication -->

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
    <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr></tbody>
  </table>


<!--  demo start -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>
  <tr onmouseout="bag_stop()" onmouseover="bag_start()">

    <td width="33%" valign="center" align="center">
      <div id="bag_input" class="hidden" style="display: inline;">
        <img src="imgs/bag_before.png" width="100%">
      </div>
      <div id="bag_animate" style="display: none;">
        <a href="imgs/bag_after.gif">
          <img src="imgs/bag_after.gif" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function bag_start() {
          document.getElementById("bag_animate").style.display = "inline";
          document.getElementById("bag_input").style.display = "none";
        }

        function bag_stop() {
          document.getElementById("bag_animate").style.display = "none";
          document.getElementById("bag_input").style.display = "inline";
        }
        bag_stop();
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://ieeexplore.ieee.org/document/10758319" id="bagMani">
          <heading> Bimanual Deformable Bag Manipulation Using a Structure-of-Interest Based Latent Dynamics Model</heading>
        </a><br>

        <b>Peng Zhou</b>,
        Pai Zheng, Jiaming Qi, Chenxi Li, Hoi-yin Lee, Chenguang Yang, David Navarro-Alarcon, Jia Pan<sup>&#8224</sup>
        <br>
        IEEE/ASME Transactions on Mechatronics (T-Mech), 2024 <br>
        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://arxiv.org/abs/2401.11432">arXiv</a> |
        <a href="https://sites.google.com/view/bagbot">project page</a> |
      </p>
      <p> This paper introduces a novel approach to deformable object manipulation (DOM) by emphasizing the identification and manipulation of structures of interest (SOIs) in deformable fabric bags. We propose a bimanual manipulation framework that leverages a graph neural network (GNN)-based latent dynamics model to succinctly represent and predict the behavior of these SOIs.
      </p>
    </td>

  </tr>
  </tbody></table>

  <!--  demo end  -->



<!--IP4DOM-->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>

  <tr onmouseout="ip4_stop()" onmouseover="ip4_start()">
    <td width="33%" valign="center" align="center">
      <div id="ip4_input" class="hidden" style="display: inline;">
        <img src="imgs/ip4_before.png" width="100%">
      </div>
      <div id="ip4_animate" style="display: none;">
        <a href="imgs/ip4_after.gif">
          <img src="imgs/ip4_after.gif" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function ip4_start() {
          document.getElementById("ip4_animate").style.display = "inline";
          document.getElementById("ip4_input").style.display = "none";
        }

        function ip4_stop() {
          document.getElementById("ip4_animate").style.display = "none";
          document.getElementById("ip4_input").style.display = "inline";
        }
        ip4_stop()
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://ieeexplore.ieee.org/document/10605907" id="ip4dom">
          <heading> Interactive Perception for Deformable Object Manipulation</heading>
        </a><br>

        Zehang Weng*, <b>Peng Zhou*<sup>&#8224</sup></b>, Hang Yin, Alexander Kravberg, Anastasiia Varava, David Navarro-Alarcon, Danica Kragic <br>
        IEEE Robotics and Automation Letters (RA-L), 2024 <br>

        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://arxiv.org/abs/2403.05177">arXiv</a> |
<!--        <a href="https://humanoid-transformer.github.io/">project page</a> |-->
      </p>
      <p>
        In this work, we address such a problem with a setup involving both an active camera and an object manipulator. Our approach is based on a sequential decision-making framework and explicitly considers the motion regularity and structure in coupling the camera and manipulator.
      </p>
    </td>

  </tr>
  </tbody></table>


<!--  HRC-RCIM -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>

  <tr onmouseout="hrc_stop()" onmouseover="hrc_start()">
    <td width="33%" valign="center" align="center">
      <div id="hrc_input" class="hidden" style="display: inline;">
        <img src="imgs/hrc_before.png" width="100%">
      </div>
      <div id="hrc_animate" style="display: none;">
        <a href="imgs/hrc_after.gif">
          <img src="imgs/hrc_after.gif" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function hrc_start() {
          document.getElementById("hrc_animate").style.display = "inline";
          document.getElementById("hrc_input").style.display = "none";
        }

        function hrc_stop() {
          document.getElementById("hrc_animate").style.display = "none";
          document.getElementById("hrc_input").style.display = "inline";
        }
        hrc_stop()
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://www.sciencedirect.com/science/article/pii/S0736584524000139" id="reactivePath">
          <heading>Reactive human‚Äìrobot collaborative manipulation of deformable linear objects using a new topological latent control model</heading>
        </a><br>

        <b>Peng Zhou</b>, Pai Zheng, Jiaming Qi<sup>&#8224</sup>, Chengxi Li, Hoi-Yin Lee, Anqing Duan, Liang Lu, Zhongxuan Li, Luyin Hu, David Navarro-Alarcon <br>
        Robotics and Computer-Integrated Manufacturing (RCIM), 2024
        <br>

        <b>ESI Highly Cited + Hot Paper</b> <br>

        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://www.sciencedirect.com/science/article/pii/S0736584524000139">arXiv</a> |
        <a href="https://sites.google.com/view/hrc-dom">project page</a> |
      </p>
      <p>
        In this paper, a novel approach is proposed for real-time reactive deformable linear object manipulation in the context of human‚Äìrobot collaboration. The proposed approach combines a topological latent representation and a fixed-time sliding mode controller to enable seamless interaction between humans and robots.
      </p>
    </td>

  </tr>
  </tbody></table>

  <center>
    <a href="javascript:toggleblock(&#39;old_papers&#39;)"> <div valign="top"> <heading>---- show more ---- </heading> </div> </a>
  </center>
  <br>
  <div id="old_papers" style="display: none;">

<!--  Second-->

  <!-- RCIM -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>

  <tr onmouseout="tool_stop()" onmouseover="tool_start()">
    <td width="33%" valign="center" align="center">
      <div id="tool_input" class="hidden" style="display: inline;">
        <img src="imgs/tool_before.png" width="100%">
      </div>
      <div id="tool_animate" style="display: none;">
        <a href="imgs/tool_after.gif">
          <img src="imgs/tool_after.gif" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function tool_start() {
          document.getElementById("tool_animate").style.display = "inline";
          document.getElementById("tool_input").style.display = "none";
        }

        function tool_stop() {
          document.getElementById("tool_animate").style.display = "none";
          document.getElementById("tool_input").style.display = "inline";
        }
        tool_stop()
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://ieeexplore.ieee.org/document/10378708" id="imitateFolding">
          <heading> Imitating tool-based garment folding from a single visual observation using hand-object graph dynamics</heading>
        </a><br>

        <b>Peng Zhou</b>,
        Jiaming Qi,
        Anqing Duan,
        Shengzeng Huo,
        Zeyu Wu,
        David Navarro-Alarcon <br>
        IEEE Transactions on Industrial Informatics (T-II), 2024 <br>

        <b>ESI Highly Cited </b> <br>

        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://ieeexplore.ieee.org/document/10378708">arXiv</a> |
        <!-- <a href="https://humanoid-transformer.github.io/">project page</a> | -->
      </p>
      <p>
        In this article, we propose a novel method of learning from demonstrations that enables robots to autonomously manipulate an assistive tool to fold garments. In contrast to traditional methods (that rely on low-level pixel features), our proposed solution uses a dense visual descriptor to encode the demonstration into a high-level hand-object graph (HoG) that allows to efficiently represent the interactions between the manipulated tool and robots. 
      </p>
    </td>

  </tr>
  </tbody></table>

<!--  Third -->

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>

  <tr onmouseout="lasesom_stop()" onmouseover="lasesom_start()">
    <td width="33%" valign="center" align="center">
      <div id="lasesom_input" class="hidden" style="display: inline;">
        <img src="imgs/lasesom_before.png" width="100%">
      </div>
      <div id="lasesom_animate" style="display: none;">
        <a href="imgs/lasesom_after.png">
          <img src="imgs/lasesom_after.png" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function lasesom_start() {
          document.getElementById("lasesom_animate").style.display = "inline";
          document.getElementById("lasesom_input").style.display = "none";
        }

        function lasesom_stop() {
          document.getElementById("lasesom_animate").style.display = "none";
          document.getElementById("lasesom_input").style.display = "inline";
        }
        lasesom_stop()
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://ieeexplore.ieee.org/abstract/document/9410363" id="lasesom">
          <heading> Lasesom: A latent and semantic representation framework for soft object manipulation</heading>
        </a><br>

        <b>Peng Zhou</b>, Jihong Zhu, Shengzeng Huo, David Navarro-Alarcon <br>
        IEEE Robotics and Automation Letters (RA-L), 2021 <br>

        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://ieeexplore.ieee.org/abstract/document/9410363">arXiv</a> |
        <a href="https://sites.google.com/view/lasesom">project page</a> |
      </p>
      <p>
        In this letter, we present LaSeSOM, a new feedback latent representation framework for semantic soft object manipulation. Our new method introduces internal latent representation layers between low-level geometric feature extraction and high-level semantic shape analysis.
      </p>
    </td>

  </tr>
  </tbody></table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>

  <tr onmouseout="isa_stop()" onmouseover="isa_start()">
    <td width="33%" valign="center" align="center">
      <div id="isa_input" class="hidden" style="display: inline;">
        <img src="imgs/isa_before.png" width="100%">
      </div>
      <div id="isa_animate" style="display: none;">
        <a href="imgs/isa_after.png">
          <img src="imgs/isa_after.png" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function isa_start() {
          document.getElementById("isa_animate").style.display = "inline";
          document.getElementById("isa_input").style.display = "none";
        }

        function isa_stop() {
          document.getElementById("isa_animate").style.display = "none";
          document.getElementById("isa_input").style.display = "inline";
        }
        isa_stop()
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://www.sciencedirect.com/science/article/pii/S0019057824002180" id="isa">
          <heading> Model predictive manipulation of compliant objects with multi-objective optimizer and adversarial network for occlusion compensation</heading>
        </a><br>

        <b>Peng Zhou</b>,
        <a href='https://people.eecs.berkeley.edu/~ilija/'>Pai Zheng</a>,
        <a href="">Jiaming Qi<sup>&#8224</sup></a>,
        <a href="">Chengxi Li</a>,
        <a href="">Hoi-Yin Lee</a>,
        <a href="">Anqing Duan</a>,
        <a href="">Liang Lu</a>,
        <a href="">Zhongxuan Li</a>,
        <a href="">Luyin Hu</a>,
        <a href="">David Navarro-Alarcon</a> <br>
        Robotics and Computer-Integrated Manufacturing (RCIM), 2024 <br>

        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://arxiv.org/abs/2205.09987">arXiv</a> |
        <!-- <a href="https://humanoid-transformer.github.io/">project page</a> | -->
      </p>
      <p>
        In this paper, we propose a new vision-based controller to automatically regulate the shape of compliant objects with robotic arms. Our method uses an efficient online surface/curve fitting algorithm that quantifies the object's geometry with a compact vector of features; This feedback-like vector enables to establish an explicit shape servo-loop. 
      </p>
    </td>

  </tr>
  </tbody></table>


  <!-- neuralReact -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>

  <tr onmouseout="neuralpath_stop()" onmouseover="neuralpath_start()">
    <td width="33%" valign="center" align="center">
      <div id="neuralpath_input" class="hidden" style="display: inline;">
        <img src="imgs/neuralpath_before.png" width="100%">
      </div>
      <div id="neuralpath_animate" style="display: none;">
        <a href="imgs/neuralpath_after.gif">
          <img src="imgs/neuralpath_after.gif" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function neuralpath_start() {
          document.getElementById("neuralpath_animate").style.display = "inline";
          document.getElementById("neuralpath_input").style.display = "none";
        }

        function neuralpath_stop() {
          document.getElementById("neuralpath_animate").style.display = "none";
          document.getElementById("neuralpath_input").style.display = "inline";
        }
        neuralpath_stop()
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://www.sciencedirect.com/science/article/pii/S0736584522002009" id="neuralReact">
          <heading> Neural reactive path planning with Riemannian motion policies for robotic silicone sealing.</heading>
        </a><br>

        <b>Peng Zhou</b>,
        <a href="">Pai Zheng</a>,
        <a href="">Jiaming Qi<sup>&#8224</sup></a>,
        <a href="">Chengxi Li</a>,
        <a href="">Hoi-Yin Lee</a>,
        <a href="">Anqing Duan</a>,
        <a href="">Liang Lu</a>,
        <a href="">Zhongxuan Li</a>,
        <a href="">Luyin Hu</a>,
        <a href="">David Navarro-Alarcon</a> <br>
        Robotics and Computer-Integrated Manufacturing (RCIM), 2022 <br>

        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://www.sciencedirect.com/science/article/pii/S0736584522002009">arXiv</a> |
        <a href="https://sites.google.com/view/reactive-sealing">project page</a> |
      </p>
      <p>
        In this paper, we present the development of a new method to automate silicone sealing with robotic manipulators. To this end, we propose a novel neural path planning framework that leverages fractional-order differentiation for robust seam detection with vision and a Riemannian motion policy for effectively learning the manipulation of a sealing gun.
      </p>
    </td>

  </tr>
  </tbody></table>





  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="15"><tbody>

  <tr onmouseout="welding_stop()" onmouseover="welding_start()">
    <td width="33%" valign="center" align="center">
      <div id="welding_input" class="hidden" style="display: inline;">
        <img src="imgs/welding_before.png" width="100%">
      </div>
      <div id="welding_animate" style="display: none;">
        <a href="imgs/welding_after.gif">
          <img src="imgs/welding_after.gif" width="100%">
        </a>
      </div>
      <script charset="utf-8" type="text/javascript">
        function welding_start() {
          document.getElementById("welding_animate").style.display = "inline";
          document.getElementById("welding_input").style.display = "none";
        }

        function welding_stop() {
          document.getElementById("welding_animate").style.display = "none";
          document.getElementById("welding_input").style.display = "inline";
        }
        welding_stop()
      </script>
    </td>

    <td width="67%" valign="top">
      <p>
        <a href="https://ieeexplore.ieee.org/document/9394722" id="welding">
          <heading> Path planning with automatic seam extraction over point cloud models for robotic arc welding</heading>
        </a><br>

        <b>Peng Zhou</b>,
        <a href=''>Pai Zheng</a>,
        <a href="">Jiaming Qi<sup>&#8224</sup></a>,
        <a href="">Chengxi Li</a>,
        <a href="">Hoi-Yin Lee</a>,
        <a href="">Anqing Duan</a>,
        <a href="">Liang Lu</a>,
        <a href="">Zhongxuan Li</a>,
        <a href="">Luyin Hu</a>,
        <a href="">David Navarro-Alarcon</a> <br>
        IEEE Robotics and Automation Letters (RA-L), 2021 <br>

        *, <sup>&#8224</sup>: equal contribution, corresponding author<br>
        | <a href="https://ieeexplore.ieee.org/document/9394722">arXiv</a> |
        <a href="https://sites.google.com/view/path-planning">project page</a> |
      </p>
      <p>
        We present a sim-to-real learning-based approach for real-world humanoid locomotion.
        To the best of our knowledge, this is the first demonstration of a fully learning-based method for real-world full-sized humanoid locomotion.
      </p>
    </td>

  </tr>
  </tbody></table>

</div>


  <!-- award -->

  <table width="100%" align="center" border="0" cellpadding="10">
    <tbody>


    <tr>
      <sectionheading>&nbsp;&nbsp;Awards</sectionheading>
      &nbsp;
      
      <td width="30%" valign="center">
        <img src="imgs/zhuhai.jpg" width="100%" style="border-radius:20px">
      </td>

      <td width="70%">
      <ul>
        <li><b>Track 3 champion, Zhuhai International Dexterous Manipulation Challenge</b>, 2024</li>
        <li><b>IEEE R10 Outstanding Volunteer Award</b>, 2023</li>
        <li><b>Outstanding Young Researcher</b>, National Engineering Research Center, 2022</li>
        <li><b>IEEE MGA Young Professional Achievement Award</b>, 2022</li>
        <li><b>Best Artificial Intelligence Application Award</b>,Hong Kong AI Open Competition, 2022</li>
        <li><b>Hong Kong Innovation and Technology Commission
          Research Talent Hub (RTH-ITF)</b>,2022 </li>
        <li><b>IEEE Young Professional</b>, 2022</li>
        <li><b>Outstanding Employee Award</b>, Tencent, 2018</li>
        <li><b>Outstanding Graduate</b>, Tongji University, 2017</li>
        <li><b>National Scholarship</b>, Ministry of Education, China, 2016</li>

<!--        <li><b></b>, </li>-->
      </ul>
    </td>

    </tr></tbody>
  </table>

  <!-- service -->

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody>
    <tr>
      <td>
        <sectionheading>Service</sectionheading>
      </td>
    </tr>
    <tr>
      <td>
        <ul style="margin-left: 5%;">
          <li>IEEE Robotics and Automation Letters (RA-L), Associate Editor</li>
          <li>
            Workshop on Contact and Impact-aware Manipulation (IROS 2025), Main Organizer<br>
            <a href="https://sites.google.com/view/iros-2025-workshop-cim">https://sites.google.com/view/iros-2025-workshop-cim</a>
          </li>
          <li>Frontiers in Robotics and AI, Special Issue on Enhancing robotic dexterity through advanced learning and multimodal perception, Leading Guest Editor, to appear</li>
          <li>Robot Learning, Youth Editorial Board Member</li>
          <li>Robot Learning, Guest Editor of Special Issue on Human-Robot Interaction and Human-Centered Robotics</li>
          <li>Electronics, Special Issue on Advancements in Robotics: Perception, Manipulation, and Interaction, Leading Guest Editor</li>
          <li>IEEE International Conference on Robotics and Biomimetics (ROBIO 2023), Associate Editor</li>
          <li>Metaverse Workshop on Elevating C3 Strategies in Robotics (ICRA 2024), Organizer</li>
          <!-- <li>
            Nature Communications, IEEE Transactions on Robotics (T-RO), The International Journal of Robotics Research (IJRR), IEEE/ASME Transactions on Mechatronics (T-Mech), IEEE Transactions on Automation Science and Engineering (T-ASE), IEEE Robotics and Automation Letters (RA-L), IEEE Transactions on Industrial Informatics (T-II), Robotics and Computer-Integrated Manufacturing (RCIM), Robotics Conferences (RSS, ICRA, IROS, ROBIO), Reviewer
          </li> -->
          <!-- <li>
            IEEE Robotics and Automation Society (RAS), IEEE RAS on Robotic Hands, Grasping and Manipulation / Human-Robot Interaction &amp; Coordination / Robot Learning / Cognitive Robotics, Member
          </li> -->
        </ul>
      </td>
    </tr>
  </tbody>
</table>

  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
    <tbody><tr><td><sectionheading>&nbsp;&nbsp;Service</sectionheading>
    <tr>
      <td width="15%" valign="center" align="center"><img style="display: inline" width="100%" src="imgs/polyu-logo.png"></td>
      <td width="85%" valign="center">
        Teaching Faculty, Perceptual Robotics (ME41006) (20-21 spring)
        <br><br>
        Teaching Faculty, Reinforcement Learning for Robotics (21-22 fall)</a>
      </td>
    </tr>
    </tbody></table> -->

  <!-- contact -->

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tbody>
            <tr>
              <td>
                <sectionheading>Contact</sectionheading>
              </td>
            </tr>
            <tr>
              <td>
                <p style="margin-left: 5%;">
        No. 16 University Road, <br>
        Songshan Lake, Dongguan, <br>
        Guangdong, China
      </p>
              </td>
            </tr>
          </tbody>
        </table>






  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody><tr><td><br>
    <p align="right"><font size="2">
      <b>Website design</b>: <a href="http://www.cs.berkeley.edu/~barron/">‚ú©</a> <a href="https://people.eecs.berkeley.edu/~pathak/">‚ú©</a>
      <a href="https://tetexiao.com/">‚ú©</a>
      <br>
      <b>Avatar photo</b>: generated in July 2024 by an AI app <a href="https://www.miaoya.cn/">Miaoya Camera</a>.
    </font></p>
  </td></tr></tbody></table>

</td></tr></tbody></table>

</body></html>
